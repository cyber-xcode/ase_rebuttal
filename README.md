We thank all reviewers for their insightful comments.

**Common-Answer**

**Common-Q1 (Reviewer-A, B)**: Motivation of detecting AI-Generated Code.\
**Common-A1**: While we could control the training data for selecting permissive-licensed and open-source data, the quality of the output code is still important especially when it is misused in some critical scenarios. We list some key motivations below:

1) *Quality and Reliability*: Existing studies have demonstrated that AI-generated code often does not meet the same standards of quality and reliability as human-written code, potentially including vulnerabilities [1,2,3] or biases [4], which necessitates human review.
2) *Regulatory Compliance*: Some regulations require transparency about the use of AI-generated content.
3) *Academic Integrity*: Detecting AI-generated code helps maintain academic integrity by ensuring that submissions are the students' own work.
4) *Accountability*: In the event of failures or issues, understanding whether the code was generated by AI can help trace the root cause and determine accountability.
Our study aims to reveal the shortcomings of current detection tools and the associated difficulties, thereby encouraging more advanced research in this area.
We will strengthen the motivations in our paper.

**Common-Q2 (Reviewer-A, C)** :  Implications.\
**Common-A2**: Thanks. We will add comprehensive implications of the paper from the perspectives of various stakeholders, including software developers, managers concerned with code quality, academic students and teachers, policymakers and regulatory bodies, LLM developers (e.g., adding watermarking), and researchers focused on AI-generated content detection (e.g., research directions).


**Review-#611A**

**ReviewA-Q1**: Relationship between the model generation and the ability to detect its content? e.g., GPT-2 VS GPT-4?\
**ReviewA-A1**:  Yes, we believe such a relationship exists. The performance of detection tools is influenced by the generation capabilities of the model. Stronger models, like GPT-4, produce content that is diverse, natural, coherent, logically consistent, and free of grammatical and syntactical errors, making it more challenging to detect. Conversely, weaker models often exhibit obvious output patterns, flaws, or features, making detection easier. Please refer to Table-5, where the results show that the content generated by GPT-3.5 (a better model) is more difficult to detect.

We will add the discussions in the paper.


**ReviewA-Q2**: Do you think the goal of detecting AI generated content is a futile effort considering the fast pace of LLM development …?\
**ReviewA-A2**:   It appears that the difficulty of detecting AI-generated contents is increasing (see **A1**) and the current detection methods may no longer work in the future, but given the importance of this problem (see **Common-A1**), we believe that continuous efforts should be invested into improving the detection methods. We also think, if AIGC reaches a point where detection is no longer technically possible, governance policies should be in-place to enforce them being identified in scenarios where they can potentially be abused.


**ReviewA-Q3**: More recent models are not available.\
**ReviewA-A3**: Due to budget constraints (e.g., GPT-4 and GPT-4o are more expensive), we did not include them in the paper. In the future, we will generate a small amount of data from GPT-4 and GPT-4o for further evaluation.


**ReviewA-Q4**:  LLMs occasionally parrot content from their training data… Is there a way to know the source of content?\
**ReviewA-A4**:  Theoretically, membership inference attacks (MIA) could be used to determine the source of the content. However, to the best of our knowledge, MIA is quite challenging in LLMs due to the output and model complexity. In this paper, all content from LLMs is considered as AI-generated.
Thank you for the comments. We will discuss this point in the threat-to-validity section.

**ReviewA-Q5**:  Why commercial models outperform open-source models?\
**ReviewA-A5**: We speculate that companies have more resources to build commercial tools. For example, they can continuously optimize their technology and update the training data, which could make them perform better than open-source tools. We will discuss this in the paper.

**ReviewA-Q6**:  Implications.\
**ReviewA-A6**: Please refer to Common-Q2


**Review-#611B**


**ReviewB-Q1**: Why is detecting AIGC for code necessary?\
**ReviewB-A1**:  Thanks for the comment. Please refer to Common-A1 for the answer. 


**Review-#611C**

**ReviewC-Q1**: Is Table3 correct?\
**ReviewC-A1**: Thank you for the reminder. We sincerely apologize for these errors regarding FPR and FNR. Upon careful review, we discovered typos in the results of Fast-DetectGPT. These errors occurred because the results were newly added in the paper revision, and we copied incorrect versions to the LaTeX. We have thoroughly checked all other results and the code to ensure that the rest of the results are correct.

|          Fast-DetectGPT         |           | Q&A |        |        | Code2Doc |       |        |           | CONCODE |        |        | Doc2Code |        |        | APPS |        |        |
|:----------------------:|:---------:|:-------:|:------:|:------:|:------------:|:-----:|:------:|:---------:|:-----------:|:------:|:------:|:-----------:|:------:|:------:|:--------:|:------:|:------:|
|                        | Avg. |   AUC   |   FPR  |   FNR  |      AUC     |  FPR  |   FNR  | Avg. |     AUC     |   FPR  |   FNR  |     AUC     |   FPR  |   FNR  |    AUC   |   FPR  |   FNR  |
|      GPT-3.5-Turbo     |   90.02  |  92.22 | 12.72 | 17.70 |    97.89    | 5.92 |  8.63 |   72.88  |    54.36   | 68.13 | 15.40 |    57.65   | 57.07 | 30.49 |  59.91  | 63.30 | 19.62 |
|     WizardCoder-15B    |           |  55.90 |  3.84 | 76.44 |    96.73    | 6.91 | 11.51 |           |    92.21   | 13.20 | 17.32 |    83.03   | 27.22 | 24.03 |  71.01  | 44.85 | 24.16 |
| CodeLlama-34B-Instruct |           |  98.61 |  3.48 |  5.44 |    98.78    | 2.00 |  5.44 |           |    81.65   | 20.72 | 31.30 |    78.84   | 30.45 | 28.11 |  77.26  | 36.09 | 22.73 |


We will correct the results.

**ReviewC-Q2**: Where is the result of the human study mentioned in Conclusion?\
**ReviewC-A2**: We apologize for the oversight. The link to the human study results was missed in the paper. This occurred during the revision when we moved the human study results from paper to our website. We have made a great effort in human study. Please check our designed survey website and the human study results at: https://sites.google.com/view/nlccd/extralstudy-human-study?authuser=0. We will update the paper with this link. Thanks!



**ReviewC-Q3**:  How are the LLMs invoked?\
**ReviewC-A3**:  Our extensive experiments used the OpenAI official API with the GPT-3.5-turbo-0125 version. We made significant efforts to avoid issues with OpenAI's access restrictions. Specifically, we used 20 different accounts, each making a maximum of 3 requests per second for data generation and collection, at a cost of approximately USD $4,000.
For the open-source models WizardCoder-15B and CodeLlama-34B-Instruct, we utilized the open-source tool vLLM [5], deploying it on an NVIDIA DGX server with eight A100 GPUs. We set the temperature to 0.2 for data generation.
We will include these details in the paper.



**ReviewC-Q4**: Is the same human-generated data used for all models? \
**ReviewC-A4**: Yes. The same data is used for all models. We will clarify in the paper.


**ReviewC-Q5**: Why did you choose RoBERTa-QA in particular? \
**ReviewC-A5**: We have two main considerations: 1) RoBERTa-QA is a more popular and well-studied detector compared to other open-source detectors (much more highly cited in academic literature). 2) We found that RoBERTa-QA has poorer detection performance than others in RQ1. Therefore, in RQ3, we aim to explore the extent to which and how fine-tuning can improve the detection performance for RoBERTa-QA.


**ReviewC-Q6**: APPS-GPT. 3792 or 3729?\
**ReviewC-A6**: We apologize for the typos. It should be 3729. We will fix them in the revised version.


**ReviewC-Q7**: Little implication\
**ReviewC-A7**: Please refer to Common-Q2


**References**: 

*[1] Wang, Jiexin, Xitong Luo, Liuwen Cao, Hongkui He, Hailin Huang, Jiayuan Xie, Adam Jatowt, and Yi Cai. "Is Your AI-Generated Code Really Secure? Evaluating Large Language Models on Secure Code Generation with CodeSecEval." arXiv preprint arXiv:2407.02395 (2024).*\
*[2] Schuster, Roei, Congzheng Song, Eran Tromer, and Vitaly Shmatikov. "You autocomplete me: Poisoning vulnerabilities in neural code completion." In 30th USENIX Security Symposium (USENIX Security 21), pp. 1559-1575. 2021.*\
*[3] He, Jingxuan, and Martin Vechev. "Large language models for code: Security hardening and adversarial testing." In Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security, pp. 1865-1879. 2023.*\
*[4] Huang, Dong, Qingwen Bu, Jie Zhang, Xiaofei Xie, Junjie Chen, and Heming Cui. "Bias assessment and mitigation in llm-based code generation." arXiv preprint arXiv:2309.14345 (2023).*\
*[5] https://github.com/vllm-project/vllm*
